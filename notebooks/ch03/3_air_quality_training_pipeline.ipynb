{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c7881d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#333;\">Training Pipeline</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "\n",
    "1. Select features for the model and create a Feature View with the selected features\n",
    "2. Create training data using the feature view\n",
    "3. Train model\n",
    "4. Evaluate model performance\n",
    "5. Save model to model registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a498f102",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> 0. üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb7e97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import hopsworks\n",
    "from functions import util\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849b250-149d-4441-b822-5780b42f343a",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> 1. Select Features and Create FeatureView"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610de819",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> 1.1üì° Connection and download Feature Groups </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0f8518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1150100\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/hopsworks-api-key.txt', 'r') as file:\n",
    "    os.environ[\"HOPSWORKS_API_KEY\"] = file.read().rstrip()\n",
    "    \n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store() \n",
    "\n",
    "secrets = util.secrets_api(project.name)\n",
    "location_str = secrets.get_secret(\"SENSOR_LOCATION_JSON\").value\n",
    "location = json.loads(location_str)\n",
    "country=location['country']\n",
    "city=location['city']\n",
    "street=location['street']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72daba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature groups\n",
    "air_quality_fg = fs.get_feature_group(\n",
    "    name='air_quality',\n",
    "    version=2,\n",
    ")\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96454a",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### <span style=\"color:#ff5f27;\"> 1.2üñç Feature View Creation and Retrieving </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8542a8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (107.07s) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm25</th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m_mean</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>wind_speed_10m_max</th>\n",
       "      <th>wind_direction_10m_dominant</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2017-09-18 00:00:00+00:00</td>\n",
       "      <td>17.258249</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>17.317459</td>\n",
       "      <td>241.196808</td>\n",
       "      <td>toscana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2018-03-29 00:00:00+00:00</td>\n",
       "      <td>12.960335</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>18.892282</td>\n",
       "      <td>177.706161</td>\n",
       "      <td>toscana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2017-09-01 00:00:00+00:00</td>\n",
       "      <td>22.712418</td>\n",
       "      <td>33.199993</td>\n",
       "      <td>17.518356</td>\n",
       "      <td>253.141479</td>\n",
       "      <td>toscana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2024-07-14 00:00:00+00:00</td>\n",
       "      <td>27.195749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.799459</td>\n",
       "      <td>259.275543</td>\n",
       "      <td>toscana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2022-11-18 00:00:00+00:00</td>\n",
       "      <td>14.481168</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>24.216522</td>\n",
       "      <td>235.768890</td>\n",
       "      <td>toscana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2018-10-02 00:00:00+00:00</td>\n",
       "      <td>13.431165</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>24.336639</td>\n",
       "      <td>34.770790</td>\n",
       "      <td>toscana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2019-05-05 00:00:00+00:00</td>\n",
       "      <td>9.208249</td>\n",
       "      <td>15.300002</td>\n",
       "      <td>21.629978</td>\n",
       "      <td>340.533997</td>\n",
       "      <td>toscana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2023-07-24 00:00:00+00:00</td>\n",
       "      <td>29.174911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.110207</td>\n",
       "      <td>184.305832</td>\n",
       "      <td>toscana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2023-12-30 00:00:00+00:00</td>\n",
       "      <td>9.818666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.673221</td>\n",
       "      <td>131.633469</td>\n",
       "      <td>toscana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2019-10-09 00:00:00+00:00</td>\n",
       "      <td>17.241587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.790833</td>\n",
       "      <td>192.063522</td>\n",
       "      <td>toscana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pm25                      date  temperature_2m_mean  precipitation_sum  \\\n",
       "0  95.0 2017-09-18 00:00:00+00:00            17.258249           9.100000   \n",
       "1  95.0 2018-03-29 00:00:00+00:00            12.960335           2.200000   \n",
       "2  95.0 2017-09-01 00:00:00+00:00            22.712418          33.199993   \n",
       "3  95.0 2024-07-14 00:00:00+00:00            27.195749           0.000000   \n",
       "4  95.0 2022-11-18 00:00:00+00:00            14.481168           3.300000   \n",
       "5  95.0 2018-10-02 00:00:00+00:00            13.431165          20.000000   \n",
       "6  95.0 2019-05-05 00:00:00+00:00             9.208249          15.300002   \n",
       "7  95.0 2023-07-24 00:00:00+00:00            29.174911           0.000000   \n",
       "8  95.0 2023-12-30 00:00:00+00:00             9.818666           0.000000   \n",
       "9  95.0 2019-10-09 00:00:00+00:00            17.241587           0.000000   \n",
       "\n",
       "   wind_speed_10m_max  wind_direction_10m_dominant     city  \n",
       "0           17.317459                   241.196808  toscana  \n",
       "1           18.892282                   177.706161  toscana  \n",
       "2           17.518356                   253.141479  toscana  \n",
       "3           14.799459                   259.275543  toscana  \n",
       "4           24.216522                   235.768890  toscana  \n",
       "5           24.336639                    34.770790  toscana  \n",
       "6           21.629978                   340.533997  toscana  \n",
       "7           23.110207                   184.305832  toscana  \n",
       "8            9.673221                   131.633469  toscana  \n",
       "9           15.790833                   192.063522  toscana  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features for training data.\n",
    "selected_features = air_quality_fg.select(['pm25']).join(weather_fg.select_all(), on=['city'])\n",
    "selected_features.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e7d24",
   "metadata": {},
   "source": [
    "#### Feature Views\n",
    "\n",
    "`Feature Views` are selections of features from different **Feature Groups** that make up the input and output API (or schema) for a model. A **Feature Views** can create **Training Data** and also be used in Inference to retrieve inference data.\n",
    "\n",
    "The Feature Views allows a schema in form of a query with filters, defining a model target feature/label and additional transformation functions (declarative feature encoding).\n",
    "\n",
    "In order to create Feature View we can use `FeatureStore.get_or_create_feature_view()` method.\n",
    "\n",
    "You can specify the following parameters:\n",
    "\n",
    "- `name` - name of a feature group.\n",
    "\n",
    "- `version` - version of a feature group.\n",
    "\n",
    "- `labels`- our target variable.\n",
    "\n",
    "- `transformation_functions` - declarative feature encoding (not used here)\n",
    "\n",
    "- `query` - selected features/labels for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9d3b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1150100/fs/1140803/fv/air_quality_fv/version/1\n"
     ]
    }
   ],
   "source": [
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='air_quality_fv',\n",
    "    description=\"weather features with air quality as the target\",\n",
    "    version=1,\n",
    "    labels=['pm25'],\n",
    "    query=selected_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ab9f4",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">1.3 Split the training data into train/test data sets </span>\n",
    "\n",
    "We use a time-series split here, with training data before this date `start_date_test_data` and test data after this date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c79be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_test_data = \"2024-10-15\"\n",
    "# Convert string to datetime object\n",
    "test_start = datetime.strptime(start_date_test_data, \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af43a52",
   "metadata": {},
   "outputs": [
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1150100/featurestores/1140803/featureview/air_quality_fv/version/1/trainingdatasets). Server response: \nHTTP code: 400, HTTP reason: Bad Request, body: b'{\"errorCode\":270172,\"usrMsg\":\"Failed to define time series split because event time column is not available in one or more feature groups.\",\"errorMsg\":\"Event time feature not found\"}', error code: 270172, error msg: Event time feature not found, user msg: Failed to define time series split because event time column is not available in one or more feature groups.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_start\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab1\\lib\\site-packages\\hsfs\\usage.py:212\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    211\u001b[0m     exception \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab1\\lib\\site-packages\\hsfs\\usage.py:208\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# Call the original method\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab1\\lib\\site-packages\\hsfs\\feature_view.py:2394\u001b[0m, in \u001b[0;36mFeatureView.train_test_split\u001b[1;34m(self, test_size, train_start, train_end, test_start, test_end, description, extra_filter, statistics_config, read_options, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[0;32m   2372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_train_test_split(\n\u001b[0;32m   2373\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size, train_end\u001b[38;5;241m=\u001b[39mtrain_end, test_start\u001b[38;5;241m=\u001b[39mtest_start\n\u001b[0;32m   2374\u001b[0m )\n\u001b[0;32m   2375\u001b[0m td \u001b[38;5;241m=\u001b[39m training_dataset\u001b[38;5;241m.\u001b[39mTrainingDataset(\n\u001b[0;32m   2376\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   2377\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2392\u001b[0m     extra_filter\u001b[38;5;241m=\u001b[39mextra_filter,\n\u001b[0;32m   2393\u001b[0m )\n\u001b[1;32m-> 2394\u001b[0m td, df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2395\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mTrainingDatasetSplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingDatasetSplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTEST\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2404\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2405\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented version to `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(td\u001b[38;5;241m.\u001b[39mversion),\n\u001b[0;32m   2406\u001b[0m     util\u001b[38;5;241m.\u001b[39mVersionWarning,\n\u001b[0;32m   2407\u001b[0m )\n\u001b[0;32m   2408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab1\\lib\\site-packages\\hsfs\\core\\feature_view_engine.py:298\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_training_data\u001b[1;34m(self, feature_view_obj, read_options, splits, training_dataset_obj, training_dataset_version, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_event_time(feature_view_obj, training_dataset_obj)\n\u001b[1;32m--> 298\u001b[0m     td_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_training_data_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# check splits\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(splits) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(td_updated\u001b[38;5;241m.\u001b[39msplits):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab1\\lib\\site-packages\\hsfs\\core\\feature_view_engine.py:667\u001b[0m, in \u001b[0;36mFeatureViewEngine._create_training_data_metadata\u001b[1;34m(self, feature_view_obj, training_dataset_obj)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_training_data_metadata\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_view_obj, training_dataset_obj):\n\u001b[1;32m--> 667\u001b[0m     td \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m     td\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m feature_view_obj\u001b[38;5;241m.\u001b[39mschema\n\u001b[0;32m    671\u001b[0m     td\u001b[38;5;241m.\u001b[39mtransformation_functions \u001b[38;5;241m=\u001b[39m feature_view_obj\u001b[38;5;241m.\u001b[39mtransformation_functions\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab1\\lib\\site-packages\\hsfs\\core\\feature_view_api.py:190\u001b[0m, in \u001b[0;36mFeatureViewApi.create_training_dataset\u001b[1;34m(self, name, version, training_dataset_obj)\u001b[0m\n\u001b[0;32m    187\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_training_data_base_path(name, version)\n\u001b[0;32m    188\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m training_dataset_obj\u001b[38;5;241m.\u001b[39mupdate_from_response_json(\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab1\\lib\\site-packages\\hsfs\\decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[1;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(inst, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab1\\lib\\site-packages\\hsfs\\client\\base.py:176\u001b[0m, in \u001b[0;36mClient._send_request\u001b[1;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[0;32m    171\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_token_expired(\n\u001b[0;32m    172\u001b[0m         request, stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTOKEN_EXPIRED_RETRY_INTERVAL, \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    173\u001b[0m     )\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1150100/featurestores/1140803/featureview/air_quality_fv/version/1/trainingdatasets). Server response: \nHTTP code: 400, HTTP reason: Bad Request, body: b'{\"errorCode\":270172,\"usrMsg\":\"Failed to define time series split because event time column is not available in one or more feature groups.\",\"errorMsg\":\"Event time feature not found\"}', error code: 270172, error msg: Event time feature not found, user msg: Failed to define time series split because event time column is not available in one or more feature groups."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = feature_view.train_test_split(\n",
    "    test_start=test_start\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.sort_values(by='date', ascending=False)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce1066-c35b-4f8b-9998-4d7426ba8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the index columns - 'date' (event_time) and 'city' (primary key)\n",
    "\n",
    "train_features = X_train.drop(['date', 'city'], axis=1)\n",
    "test_features = X_test.drop(['date', 'city'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bed512",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f7b68",
   "metadata": {},
   "source": [
    "The `Feature View` is now saved in Hopsworks and you can retrieve it using `FeatureStore.get_feature_view(name='...', version=1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b12d6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46611b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">2. Modeling</span>\n",
    "\n",
    "We will train a regression model to predict pm25 using our 4 features (wind_speed, wind_dir, temp, precipitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb637114-bcc8-499a-9b5a-55d63cfff68e",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> 2.1 Training the model</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be358ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating an instance of the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor()\n",
    "\n",
    "# Fitting the XGBoost Regressor to the training data\n",
    "xgb_regressor.fit(train_features, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22cf4a-4ca3-4c63-a20c-45ba99e5e59d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:#ff5f27;\">2.2 Evaluating </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e1beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting target values on the test set\n",
    "y_pred = xgb_regressor.predict(test_features)\n",
    "\n",
    "# Calculating Mean Squared Error (MSE) using sklearn\n",
    "mse = mean_squared_error(y_test.iloc[:,0], y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "# Calculating R squared using sklearn\n",
    "r2 = r2_score(y_test.iloc[:,0], y_pred)\n",
    "print(\"R squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = y_test\n",
    "df['predicted_pm25'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = X_test['date']\n",
    "df = df.sort_values(by=['date'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574bf88-98d9-4b6a-b51b-388797a4e0e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:#ff5f27;\">2.3 Exporting the model</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3100f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a directory for the model artifacts if it doesn't exist\n",
    "model_dir = \"air_quality_model\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "images_dir = model_dir + \"/images\"\n",
    "if not os.path.exists(images_dir):\n",
    "    os.mkdir(images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4485b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = images_dir + \"/pm25_hindcast.png\"\n",
    "plt = util.plot_air_quality_forecast(city, street, df, file_path, hindcast=True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38888e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting feature importances using the plot_importance function from XGBoost\n",
    "plot_importance(xgb_regressor, max_num_features=4)\n",
    "feature_importance_path = images_dir + \"/feature_importance.png\"\n",
    "plt.savefig(feature_importance_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c69ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b177983",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <span style='color:#ff5f27'>3. üóÑSaving Model Registry</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where you can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db096e82",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚öôÔ∏è Model Schema</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82881cab",
   "metadata": {},
   "source": [
    "The model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/machine-learning-api/latest/generated/model_schema/), which describes the inputs and outputs for a model.\n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f1ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "# Creating input and output schemas using the 'Schema' class for features (X) and target variable (y)\n",
    "input_schema = Schema(X_train)\n",
    "output_schema = Schema(y_train)\n",
    "\n",
    "# Creating a model schema using 'ModelSchema' with the input and output schemas\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "# Converting the model schema to a dictionary representation\n",
    "schema_dict = model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e3fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving the XGBoost regressor object as a json file in the model directory\n",
    "xgb_regressor.save_model(model_dir + \"/model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = { \n",
    "        \"MSE\": str(mse),\n",
    "        \"R squared\": str(r2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81434864",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()\n",
    "\n",
    "# Creating a Python model in the model registry named 'air_quality_xgboost_model'\n",
    "\n",
    "aq_model = mr.python.create_model(\n",
    "    name=\"air_quality_xgboost_model\", \n",
    "    metrics= res_dict,\n",
    "    model_schema=model_schema,\n",
    "    input_example=X_test.sample().values, \n",
    "    description=\"Air Quality (PM2.5) predictor\",\n",
    ")\n",
    "\n",
    "# Saving the model artifacts to the 'air_quality_model' directory in the model registry\n",
    "aq_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b3088",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 04: Batch Inference</span>\n",
    "\n",
    "In the following notebook you will use your model for Batch Inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd9154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lab1)",
   "language": "python",
   "name": "lab1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "63265f9757e7c73c149a91832e3b2b12ced37a5390b9151ad08a04f276cd5846"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
